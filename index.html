<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="数据挖掘日常笔记" type="application/atom+xml">






<meta name="description" content="Every failure is leading towards success.">
<meta name="keywords" content="深度学习 机器学习 数据挖掘 推荐 DKT ITS">
<meta property="og:type" content="website">
<meta property="og:title" content="数据挖掘日常笔记">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="数据挖掘日常笔记">
<meta property="og:description" content="Every failure is leading towards success.">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据挖掘日常笔记">
<meta name="twitter:description" content="Every failure is leading towards success.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>数据挖掘日常笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband">

    </div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">数据挖掘日常笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Zoe的博客 | Zoe Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/19/推荐系统-找相似/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/19/推荐系统-找相似/" itemprop="url">推荐系统-找相似</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-19T16:11:10+08:00">
                2019-09-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-09-20T15:35:52+08:00">
                2019-09-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐/" itemprop="url" rel="index">
                    <span itemprop="name">推荐</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2.2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  9
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近做推荐，召回其中的一个策略是采用协同过滤的算法，本来觉得不就是简单的一通计算相似度取topK相似用户做推荐就好嘛！但是真正做起来就要踩“大数据”的坑。</p>
<p>传统的基于协同过滤，核心在于计算相似度找到topK相似用户/物品，无论哪种计算相似度的时间复杂度都会随着用户/物品规模的扩大而呈平方增长，对于实际应用中，用户/物品往往达到千万甚至更多，因此暴力计算相似度显然不可行，这时就需要采用近似计算的方法，损失一部分精度来换取计算的效率。</p>
<p>最开始准备采用Facebook的Faiss框架来计算相似度，但是由于准备用go做服务，Faiss没有官方提供go的接口需要手动编译，而且计算用户相似度时，数据是动态的，用起来不那么方便，因此弃用Faiss转向LSH来计算用户的相似度。下面详述Min hashing和LSH（local sensitive hashing）原理。</p>
<h1 id="Hash-Functions"><a href="#Hash-Functions" class="headerlink" title="Hash Functions"></a>Hash Functions</h1><p>大家应该经常能听到哈希，也用过java类或相似的包来计算哈希值，哈希函数根据hash-key可以产生一个bucket number作为其存储位置，因此可以快速查询。hash table就是根据key-value通过hash function（记为h）得到映射的值进行寻址直接访问的数据结构。</p>
<ol>
<li>对于不同的key得到同样的散列地址，即$k_{1}≠ k_{2}$但$h_{1} = h_{2}$，我们称之为碰撞。</li>
<li>若对于所有的key经过hash function的映射到每一个地址的概率是相等的，则成为均匀散列函数。</li>
</ol>
<h1 id="Min-Hashing"><a href="#Min-Hashing" class="headerlink" title="Min Hashing"></a>Min Hashing</h1><h2 id="Jaccard相似度"><a href="#Jaccard相似度" class="headerlink" title="Jaccard相似度"></a>Jaccard相似度</h2><p>Jaccard index用于计算相似度，是距离的一种度量方式，假设有向量$S_{1}$和$S_{2}$，我们定义<center>$Jaccard(S_{1}, S_{2})=\frac{S_{1}∩S_{2}}{S_{1}∪S_{2}}$</center></p>
<h2 id="Minhash"><a href="#Minhash" class="headerlink" title="Minhash"></a>Minhash</h2><p>Min Hashing是LSH的一种，可以用于快速估计两个向量的相似度。Min Hashing和Jaccard相似度有很大的关系：</p>
<pre><code>对两个向量进行Min Hashing，产生的哈希值相同的概率等于两个向量的Jaccard相似度
                                                            -- (1)
</code></pre><p>通过MinHash得到映射分两步：</p>
<ul>
<li>首先对row进行permutation，每个向量做同样的操作</li>
<li>向量的MinHash值对应permutation后，取值为非零的第一行的row index</li>
</ul>
<p>考虑为什么通过Minhash可以达到上述(1)所说的效果呢？我们举例如下，有四个向量，每个向量有5行（维）（我们可以把列看作User，行看作Item，即User$S_{1}$购买过2，3商品…）：</p>
<p>表1:</p>
<pre><code>| $S_{1}$ |  $S_{2}$  | $S_{3}$ | $S_{4}$  
</code></pre><p>-|-|-|-|-<br>0 | 1 | 0 | 0 | 1 |<br>1 | 0 | 0 | 1 | 0 |<br>2 | 0 | 1 | 0 | 1 |<br>3 | 1 | 0 | 1 | 1 |<br>4 | 0 | 0 | 1 | 0 |</p>
<p>我们看$S_{1}$和$S_{2}$，每一行的取值分三种情况：</p>
<ol>
<li>$S_{1}$和$S_{2}$同为1</li>
<li>$S_{1}$和$S_{2}$一个为0一个为1</li>
<li>$S_{1}$和$S_{2}$同为0</li>
</ol>
<p>由于矩阵通常很稀疏（考虑实际中用户与购买物品的矩阵），因此大部分为情况3，但是1和2决定来Jaccard($S_{1}$,$S_{2}$)和$h\left(S_{1}\right)=h\left(S_{2}\right)$概率，假设情况1有x行，情况2有y行，则$\operatorname{Jaccard}\left(S_{1}, S_{2}\right)=\frac{x}{x+y}$（x相当于$S_{1} \cap S_{2}$，y相当于$S_{1} \cup S_{2}$）,若Permutation是随机的，则$S_{1}$和$S_{2}$从上向下找第一个非零属于情况1的概括为$\frac{x}{x+y}$。 </p>
<h2 id="Min-Hashing-signatures"><a href="#Min-Hashing-signatures" class="headerlink" title="Min Hashing signatures"></a>Min Hashing signatures</h2><p>如何通过Min Hashing产生签名呢？</p>
<p>对向量做m次permutation(m一般为几百或更小，小于原向量长度n)，每一次经过permutation得到minhash记为$h_{1}, h_{2}, \ldots, h_{m}$， 则向量获得的签名可以表示为：<center>$\operatorname{sig}(A)=\left[h_{1}(A), h_{2}(A), \ldots, h_{m}(A)\right]$</center><br>对于一个高维度的向量进行permutation也是非常耗时的，因此可以随机选择m个哈希函数代替permutation。具体做法如下：</p>
<ol>
<li>取m个针对row index的哈希函数$h_{1},h_{2}, \ldots, h_{m}$将原始0,1…n-1映射到0,1…n-1上</li>
<li>记Sig(i,c)为第c列在第i个哈希函数下的MinHash值，初始值记做$\infty$</li>
<li>对于每一行r<ul>
<li>计算$h_{1}(r),h_{2}(r),\ldots,h_{m}(r)$</li>
<li>对于每列c：<ul>
<li>如果c在r行取值为0，忽略</li>
<li>如果c在r行取值为1，则对$i=1,2,…,m$计算Sig(i,c) = Min(Sig(i,c),$h_{i}(r)$)</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>对表1所示数据，我们举例说明Min Hashing签名生成过程，假设我们选择两个哈希函数对row index生成MinHash，如下表2所示：$h_{1}(x) = x+1  \mod 5$, $h_{2}(x) = 3x+1 \mod 5$</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
<th>x+1 mod 5</th>
<th>3x+1 mod 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>5</td>
<td>3</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>初始时，哈希签名如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_{1}$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td></td>
</tr>
<tr>
<td>$h_{2}$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>对于第0行，只有$S_{1}$ 和$S_{4}$为非零，且$h_{1}$和$h_{2}$均为1,则当前签名矩阵变为</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_{1}$</td>
<td>1</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>$h_{2}$</td>
<td>1</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>对于第1行，$S_{3}$为1，对于$h_{1}$和$h_{2}$为2和4，则签名矩阵变为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_{1}$</td>
<td>1</td>
<td>$\infty$</td>
<td>2</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>$h_{2}$</td>
<td>1</td>
<td>$\infty$</td>
<td>4</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>继续至第2行，$S_{2}$ 和$S_{4}$为非零，对应$h_{1}$和$h_{2}为3和2，$S_{4}$目前的值均为1，小于$h_{1}$，$h_{2}$对应的3和2，则签名矩阵变为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_{1}$</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>$h_{2}$</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>1</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>…</p>
<p>最后扫描完第4行，最后得到签名矩阵为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$S_{1}$</th>
<th>$S_{2}$</th>
<th>$S_{3}$</th>
<th>$S_{4}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_{1}$</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>$h_{2}$</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>通过上述过程可以看到，实际的$Jaccard(S_{1},S_{4})=2/3$，但通过签名计算得到的$Jaccard(S_{1},S_{4})=1$，上述结果是由于示例只选择了两个哈希函数，在实际应用中，会选择更多的哈希函数并且向量数往往在百万千万级，这时就会比较相近了。</p>
<h1 id="Local-Sensitive-Hashing-LSH"><a href="#Local-Sensitive-Hashing-LSH" class="headerlink" title="Local Sensitive Hashing(LSH)"></a>Local Sensitive Hashing(LSH)</h1><p>经过Min Hashing，我们可以实现数据降维并保证降维后的数据保持原空间的相似度，降低了在物品维度很高时计算相似度的时间复杂度，但是当用户数巨大时，计算两两相似度仍旧需要很高的时间开销，如何进一步降低寻找相似用户的复杂度，就需要LSH这样一个近似算法来实现。</p>
<p>LSH常见的做法是经过多次哈希将相似的向量散列到相同的桶中，检索时，我们只需要考虑相同桶中的任意对作为候选集即可。我们称不相似的向量被散列到相同的桶样本为false positive，相似的向量被散列到不同到桶中的样本称为false negetive。</p>
<p>在有来Min Hashing签名的基础上，一个有效的办法是将签名分段，我们称之为band。如下图所示：<br><img src="/2019/09/19/推荐系统-找相似/1.jpg" alt="1"></p>
<p>每个签名都被分成4段，向量的签名散列到对应的band上，如果band相同的越多，则其相似度越高。我们可以通过band获取candidate，计算candidate相似用户的相似度就好了。</p>
<h2 id="如何选择band数？"><a href="#如何选择band数？" class="headerlink" title="如何选择band数？"></a>如何选择band数？</h2><p>假设我们有b和band，每个band有r行，即总的Min Hashing签名长度为b*r，假设两个向量的Jaccard相似度为s，则：</p>
<ol>
<li>对于某一个band，所有行都相等的概率为：$s^r$</li>
<li>对于某一个band，至少有一行不同的概率为：$1 - s^r$</li>
<li>对所有band，至少有一行不同的概率为： $(1 - s^r)^b$</li>
<li>至少有一个band的所有行都相等的概率为： $1- (1 - s^r)^b$，即两个用户成为candidate的概率</li>
</ol>
<p>上述概率在r，b不同值时为一个S曲线，如当r=6，b=100时的曲线如下图：</p>
<p><img src="/2019/09/19/推荐系统-找相似/2.jpg" alt="2"></p>
<p>我们定义超过0.5的概率就有可能成为candidate，此时对应的相似度称为threshold:<center>$threshold \approx (\frac{1}{b})^{(\frac{1}{r})}$</center></p>
<p>在实际应用中，我们需要先确定最小相似度，即相似度大于多少我们认为可以作为candidate，然后确定哈希签名的长度进而可以确定b和r。我们需要考虑的是：</p>
<ul>
<li>我们想要尽可能少出现false negative，我们需要选择b和r使得thresh&lt; 我们定义的最小相似度。</li>
<li>如果要保证计算速度快并尽可能少出现false positive，我们需要选择b和r使得thresh&gt;最小相似度。</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h1><ol>
<li>Mining of massive datasets；  Jeffrey D. Ullman Anand Rajaraman, Jure Leskovec.</li>
<li><a href="https://zhuanlan.zhihu.com/p/46164294" target="_blank" rel="noopener">大规模数据的相似度计算：LSH算法</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/20/推荐之FM-FFM-Deep-FM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/20/推荐之FM-FFM-Deep-FM/" itemprop="url">推荐之FM/FFM/Deep_FM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-20T17:02:29+08:00">
                2019-08-20
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-20T17:07:35+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐/" itemprop="url" rel="index">
                    <span itemprop="name">推荐</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  976
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  4
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h1><p>FM常用于CTR估计，能够处理稀疏数据下的特征组合的问题。</p>
<h2 id="FM原理"><a href="#FM原理" class="headerlink" title="FM原理"></a>FM原理</h2><p>一般的线性模型不考虑特征之间的关系，公式定义如下：<br>    <center> $y=w_{0}+\sum_{i=1}^{n}x_{i}w_{i}$         ...(1) </center><br>为了表达特征之间的相关性，利用多项式函数，定义如下：<br>    <center>$y=w_{0}+\sum_{i=1}^{n}x_{i}w_{i}+\sum_{i=1}^{n}\sum_{j=i+1}^{n}w_{i,j
}x_{i}x_{j}$         ...(2)</center><br>如上述公式，定义的二阶特征参数有$\frac{n*(n-1)}{2}$个，同时当特征非常稀疏且纬度高时，不仅时间复杂度在增加，同时由于稀疏的特征，很多二阶特征系数通常学习不到。<br>为了解决上述的问题，FM对每个特征引入辅助向量$V_{i} = [v_{i1},v_{i2},…v_{ik}]^{T}$，此时每个特征学习到一个k纬向量而不是一个值，从而解决数据稀疏下到特征组合问题。此时模型如下：<center>$y = w_{0}+\sum_{i=0}^{n}w_{i}x_{i} + \sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_{i},v_{j}>x_{i}x_{j}$        ...(3) </v_{i},v_{j}></center><br>其中k表示辅助向量的纬度，为超参数。上述表达式的时间复杂度为$kn^{2}$。为降低时间复杂度，对(3)式最后一项做如下变化：<br>$\sum _ { i = 1 } ^ { n } \sum _ { j = i + 1 } ^ { n } &lt; V _ { i } , V _ { j } &gt; x _ { i } x _ { j }$<br>$= \frac { 1 } { 2 } \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { n } &lt; V _ { i } , V _ { j } &gt; x _ { i } x _ { j } - \frac { 1 } { 2 } \sum _ { i = 1 } ^ { n } &lt; V _ { i } , V _ { i } &gt; x _ { i } x _ { i }$<br>$=\frac{1}{2}(\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{f=1}^{k}v_{if}v_{jf}x_{i}x_{j} - \sum_{i=1}^{n}\sum_{f=1}^{k}v_{if}v_{if}x_ix_i)$<br>$=\frac{1}{2}\sum_{f=1}^{k}((\sum_{i=1}^{n}v_{if}x_{i})^2 - \sum_{i=1}^{n}(v_{if}x_i)^2)$</p>
<p>通过上述变化可以发现，将公式(3)的时间复杂度从$kn^{2}$降低为$kn$。</p>
<h2 id="FM优点"><a href="#FM优点" class="headerlink" title="FM优点"></a>FM优点</h2><ol>
<li>通过引入辅助向量，能够解决稀疏数据的特征组合问题。</li>
<li>模型的时间复杂度为线性的（kn）。</li>
</ol>
<h1 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h1><p>FFM(Field-aware Factorization Machine)为FM的改进版，区别是FM的辅助向量为K维，而FFM的辅助向量为F*K维，其中F为field个数。对应的FFM模型如下：<br>    <center>$y = w_{0}+\sum_{i=0}^{n}w_{i}x_{i} + \sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_{i,f_{j}},v_{j,f_{i}}>x_{i}x_{j}$        ...(3) </v_{i,f_{j}},v_{j,f_{i}}></center><br>    其中$f_{i,f_{j}}$为i特征属于$f_{j}$下的辅助向量。由于FFM不能按照(3)进行化简，因此时间复杂度为$kn^{2}$。</p>
<h2 id="FFM优缺点"><a href="#FFM优缺点" class="headerlink" title="FFM优缺点"></a>FFM优缺点</h2><ol>
<li>与FM相比FFM具有field感知能力，模型能力更强。</li>
<li>与FM相比，辅助向量变为F*K维学习参数变为FM的F倍。</li>
<li>与FM相比，时间复杂度高。</li>
</ol>
<h1 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h1><p>由于FM/FFM使用的是一阶和二阶特征，为了学习更高阶特征，Huifeng Guo等人提出了DeepFM。<br>与DeepFM相似的模型有FNN，PNN，Wide&amp;deep，模型结构如下图所示：<br><img src="/2019/08/20/推荐之FM-FFM-Deep-FM/1.png" alt="1"><br>FNN使用预训练好的权重作为NN的输入，只能得到高阶特征组合，PNN则是在embedding层和第一层隐藏层中加入了product操作。Wide&amp;deep 的提出则是为了综合低阶特征组合和高阶特征组合，wide部分多为专家经过特征工程得到的低阶组合特征，而deep部分则通过NN网络学习到高阶特征。<br>DeepFM不仅可以达到Wide&amp;deep同时学习低阶特征组合和高阶特征组合的效果，同时想比于Wide&amp;deep网络的优点是，不需要专家经过特征组合筛选特征。网络结果如下：<br><img src="/2019/08/20/推荐之FM-FFM-Deep-FM/2.png" alt="2"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction   [<a href="https://arxiv.org/pdf/1703.04247.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.04247.pdf</a>]</li>
<li>推荐系统召回四模型之：全能的FM模型<br>[<a href="https://zhuanlan.zhihu.com/p/58160982" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58160982</a>]</li>
<li>CTR预估算法之FM, FFM, DeepFM及实践<br>[<a href="https://blog.csdn.net/John_xyz/article/details/78933253" target="_blank" rel="noopener">https://blog.csdn.net/John_xyz/article/details/78933253</a>]</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/15/TF的三种模型的保存与加载方式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/15/TF的三种模型的保存与加载方式/" itemprop="url">TF的三种模型的保存与加载方式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-15T16:14:26+08:00">
                2019-08-15
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-15T20:53:02+08:00">
                2019-08-15
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">Deeplearning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  10
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TF的三种模型的保存与加载方法"><a href="#TF的三种模型的保存与加载方法" class="headerlink" title="TF的三种模型的保存与加载方法"></a>TF的三种模型的保存与加载方法</h1><p>当我们训练完一波模型，准备把模型应用于生产环境时，我们需要在生产环境部署预测的model，此时就涉及到如何把我们训练好的model重新加载以及处于效率的考虑，用什么方法更好的部署模型。因此我们来谈谈常见的三种模型的保存与加载方法。</p>
<ul>
<li>Checkpoint： 知道模型结构，单纯保存变量  </li>
<li>SavedModel： 不知道模型结构，保存模型和变量</li>
<li>Freeze pb： 不需要再改变量，只要常量化的模型（“冻结”）</li>
</ul>
<h2 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h2><p>第一种必然是大家最为熟悉的checkpoint的方式，通常在模型训练时，我们通过saver=tf.train.saver()定义saver，在session中通过saver.save()保存模型中的变量。此时我们<strong>只是单纯保存了变量而没有对模型本身做任何保存</strong>，此时恢复模型需要有模型对应的源代码，因此当我们需要在C++中恢复模型，只能用C++把模型的代码复写一遍。<br>保存:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>,<span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>,<span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">w = tf.get_variable(name=<span class="string">'w'</span>, shape=[<span class="number">2</span>, <span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line">c = tf.add(tf.add(a, b), w)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    result = sess.run(c, feed_dict=&#123;a:[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]], b:[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]]&#125;)</span><br><span class="line">    saver.save(sess, <span class="string">'./saved_model/model.ckpt'</span>)</span><br><span class="line"></span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[4. 6.]</span><br><span class="line"> [7. 9.]]</span><br></pre></td></tr></table></figure>
<p>加载：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">w = tf.get_variable(name=<span class="string">'w'</span>, shape=[<span class="number">2</span>, <span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line">c = tf.add(tf.add(a, b), w)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./saved_model/model.ckpt'</span>)</span><br><span class="line">    result = sess.run(c, feed_dict=&#123;a: [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], b: [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>]]&#125;)</span><br><span class="line"></span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[4. 6.]</span><br><span class="line"> [7. 9.]]</span><br></pre></td></tr></table></figure>
<p>可以看到，通过restore我们可以得到同样的结果。</p>
<h2 id="SavedModel"><a href="#SavedModel" class="headerlink" title="SavedModel"></a>SavedModel</h2><p>TF官网介绍说SavedModel是一种独立于语言而且可以恢复的序列化格式，使较高级别的系统和工具可以创建，使用和转换Tensorflow模型。常见的两种与SavedModel交互的方式包括tf.saved_model API 和tf.estimator.Estimator。</p>
<h3 id="tf-saved-model-API"><a href="#tf-saved-model-API" class="headerlink" title="tf.saved_model API"></a>tf.saved_model API</h3><h4 id="保存方法1-使用tf-saved-model-simple-save"><a href="#保存方法1-使用tf-saved-model-simple-save" class="headerlink" title="保存方法1:使用tf.saved_model.simple_save"></a>保存方法1:使用tf.saved_model.simple_save</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">w = tf.get_variable(name=<span class="string">'w'</span>, shape=[<span class="number">2</span>, <span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line">c = tf.add(tf.add(a, b), w)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    tf.saved_model.simple_save(session=sess,</span><br><span class="line">                               export_dir=<span class="string">'./saved_model/pb/1'</span>,</span><br><span class="line">                               inputs=&#123;<span class="string">'a'</span>: a, <span class="string">'b'</span>: b&#125;,</span><br><span class="line">                               outputs=&#123;<span class="string">'c'</span>: c&#125;)</span><br></pre></td></tr></table></figure>
<p>此时，我们可以在saved_model/pb下看到如下文件结构：</p>
<pre><code> --  saved_model
    |--  pb
        |-- 1
            |-- saved_model.pb
            |-- variables
                |-- variables.data-00000-of-00001
                |-- variables.index
</code></pre><h4 id="保存方法2-通过SavedModelBuilder构建"><a href="#保存方法2-通过SavedModelBuilder构建" class="headerlink" title="保存方法2:通过SavedModelBuilder构建"></a>保存方法2:通过SavedModelBuilder构建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">w = tf.get_variable(name=<span class="string">'w'</span>, shape=[<span class="number">2</span>, <span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line">c = tf.add(tf.add(a, b), w)</span><br><span class="line">saver =tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># sess.run(tf.global_variables_initializer())</span></span><br><span class="line">    saver.restore(sess, <span class="string">'./saved_model/model.ckpt'</span>)</span><br><span class="line">    builder = tf.saved_model.builder.SavedModelBuilder(<span class="string">"./saved_model/pb/2"</span>)</span><br><span class="line">    inputs = &#123;<span class="string">'a'</span>: tf.saved_model.utils.build_tensor_info(a),</span><br><span class="line">              <span class="string">'b'</span>: tf.saved_model.utils.build_tensor_info(b)&#125;</span><br><span class="line">    output = &#123;<span class="string">'c'</span>: tf.saved_model.utils.build_tensor_info(c)&#125;</span><br><span class="line"></span><br><span class="line">    prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(</span><br><span class="line">        inputs=inputs,</span><br><span class="line">        outputs=output,</span><br><span class="line">        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME</span><br><span class="line">    )</span><br><span class="line">    builder.add_meta_graph_and_variables(</span><br><span class="line">        sess,</span><br><span class="line">        [tf.saved_model.tag_constants.SERVING],</span><br><span class="line">        &#123;tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_signature&#125;</span><br><span class="line">    )</span><br><span class="line">    builder.save()</span><br></pre></td></tr></table></figure>
<h4 id="通过saved-model-cli命令查看SavedModel"><a href="#通过saved-model-cli命令查看SavedModel" class="headerlink" title="通过saved_model_cli命令查看SavedModel"></a>通过saved_model_cli命令查看SavedModel</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saved_model_cli show --dir /Users/xxx/Documents/pycharm_workspace/test_python/saved_model/pb/1 --all</span><br></pre></td></tr></table></figure>
<p>可以得到如下结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MetaGraphDef with tag-set: <span class="string">'serve'</span> contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'serving_default'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[<span class="string">'a'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (2, 2)</span><br><span class="line">        name: a:0</span><br><span class="line">    inputs[<span class="string">'b'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (2, 2)</span><br><span class="line">        name: b:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[<span class="string">'c'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (2, 2)</span><br><span class="line">        name: Add_1:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure></p>
<h4 id="加载方法1-通过tf-saved-model-loader-load加载"><a href="#加载方法1-通过tf-saved-model-loader-load加载" class="headerlink" title="加载方法1: 通过tf.saved_model.loader.load加载"></a>加载方法1: 通过tf.saved_model.loader.load加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">export_dir = <span class="string">'./saved_model/pb/2'</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    meta_graph_def = tf.saved_model.loader.load(sess, [<span class="string">'serve'</span>],</span><br><span class="line">                                                export_dir)</span><br><span class="line">    a = sess.graph.get_tensor_by_name(<span class="string">'a:0'</span>)</span><br><span class="line">    b = sess.graph.get_tensor_by_name(<span class="string">'b:0'</span>)</span><br><span class="line"></span><br><span class="line">    c = sess.graph.get_tensor_by_name(<span class="string">'Add_1:0'</span>)</span><br><span class="line">    print(sess.run(c, feed_dict=&#123;a: [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], b: [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>]]&#125;))</span><br></pre></td></tr></table></figure>
<h4 id="加载方法2-通过tf-contrib-predictor-from-saved-model"><a href="#加载方法2-通过tf-contrib-predictor-from-saved-model" class="headerlink" title="加载方法2: 通过tf.contrib.predictor.from_saved_model"></a>加载方法2: 通过tf.contrib.predictor.from_saved_model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">export_dir = <span class="string">'./saved_model/pb/2'</span></span><br><span class="line">predictor_fn = tf.contrib.predictor.from_saved_model(</span><br><span class="line">    export_dir=export_dir,</span><br><span class="line">    signature_def_key=<span class="string">"serving_default"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output = predictor_fn(&#123;<span class="string">'a'</span>: [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]],</span><br><span class="line">                       <span class="string">'b'</span>: [[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>]]</span><br><span class="line">                       &#125;)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<h3 id="结合tf-estimator-Estimator使用"><a href="#结合tf-estimator-Estimator使用" class="headerlink" title="结合tf.estimator.Estimator使用"></a>结合tf.estimator.Estimator使用</h3><h4 id="保存方法"><a href="#保存方法" class="headerlink" title="保存方法"></a>保存方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serving_input_receiver_fn</span><span class="params">()</span>:</span></span><br><span class="line">    feature_spec = &#123;<span class="string">'a'</span>: tf.FixedLenFeature([<span class="number">2</span>,<span class="number">2</span>], tf.float32),</span><br><span class="line">                    <span class="string">'b'</span>: tf.FixedLenFeature([<span class="number">2</span>,<span class="number">2</span>], tf.float32)&#125;</span><br><span class="line"></span><br><span class="line">    serialized_tf_example = tf.placeholder(dtype=tf.string,</span><br><span class="line">                                           shape=[<span class="number">1</span>],</span><br><span class="line">                                           name=<span class="string">'input_example_tensor'</span>)</span><br><span class="line">    receiver_tensors = &#123;<span class="string">'examples'</span>: serialized_tf_example&#125;</span><br><span class="line">    features = tf.parse_example(serialized_tf_example, feature_spec)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.estimator.export.ServingInputReceiver(features, receiver_tensors)</span><br><span class="line"></span><br><span class="line">estimator.export_savedmodel(<span class="string">'saved_model'</span>, serving_input_receiver_fn)</span><br></pre></td></tr></table></figure>
<h4 id="通过tf-serving部署服务访问"><a href="#通过tf-serving部署服务访问" class="headerlink" title="通过tf-serving部署服务访问"></a>通过tf-serving部署服务访问</h4><ul>
<li><p>部署server端：</p>
<ul>
<li>基于Dockerfile 创建镜像：<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">18.04</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install general packages</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update &amp;&amp; apt-get install -y \</span></span><br><span class="line"><span class="bash">        curl \</span></span><br><span class="line"><span class="bash">        libcurl3-dev \</span></span><br><span class="line"><span class="bash">        unzip \</span></span><br><span class="line"><span class="bash">        wget \</span></span><br><span class="line"><span class="bash">        &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apt-get clean &amp;&amp; \</span></span><br><span class="line"><span class="bash">    rm -rf /var/lib/apt/lists/*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Previous Installation of tensorflow-model-server (BROKEN RECENTLY)</span></span><br><span class="line"><span class="comment">#RUN echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list \</span></span><br><span class="line"><span class="comment">#    &amp;&amp; curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add - \</span></span><br><span class="line"><span class="comment">#    &amp;&amp; apt-get update &amp;&amp; apt-get install tensorflow-model-server</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># New installation of tensorflow-model-server</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> TEMP_DEB=<span class="string">"<span class="variable">$(mktemp)</span>"</span> \</span></span><br><span class="line"><span class="bash">    &amp;&amp; wget -O <span class="string">"<span class="variable">$TEMP_DEB</span>"</span> <span class="string">'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-1.12.0/t/tensorflow-model-server/tensorflow-model-server_1.12.0_all.deb'</span> \</span></span><br><span class="line"><span class="bash">    &amp;&amp; dpkg -i <span class="string">"<span class="variable">$TEMP_DEB</span>"</span> \</span></span><br><span class="line"><span class="bash">    &amp;&amp; rm -f <span class="string">"<span class="variable">$TEMP_DEB</span>"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gRPC port</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8500</span></span><br><span class="line"><span class="comment"># REST API port</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8501</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Serve the model when the container starts</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> tensorflow_model_server \</span></span><br><span class="line"><span class="bash">--port=8500 \</span></span><br><span class="line"><span class="bash">--rest_api_port=8501 \</span></span><br><span class="line"><span class="bash">--model_name=<span class="string">"<span class="variable">$MODEL_NAME</span>"</span> \</span></span><br><span class="line"><span class="bash">--model_base_path=<span class="string">"<span class="variable">$MODEL_PATH</span>"</span></span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行如下命令创建镜像：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --rm -f Dockerfile -t tensorflow-serving-example:0.1 .</span><br></pre></td></tr></table></figure>
<ul>
<li><p>创建临时目录保存savedModel</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ./saved_model/dkt/1</span><br><span class="line">cp -R ./saved_model/pb/* ./saved_model/<span class="built_in">test</span>/1</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动容器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it -v /home/xxx/tf_serving/saved_model/:/models -e MODEL_NAME=<span class="built_in">test</span> -e MODEL_PATH=/models/<span class="built_in">test</span> -p 8500:8500 -p 8501:8501 --name tensorflow-serving-example tensorflow-serving-example:0.1</span><br></pre></td></tr></table></figure>
<p>至此，server已启动，运行client进行测试  </p>
</li>
</ul>
</li>
<li><p>client</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.util <span class="keyword">import</span> make_tensor_proto</span><br><span class="line"><span class="keyword">from</span> tensorflow_serving.apis <span class="keyword">import</span> predict_pb2</span><br><span class="line"><span class="keyword">from</span> tensorflow_serving.apis <span class="keyword">import</span> prediction_service_pb2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(host, port, model, signature_name)</span>:</span></span><br><span class="line">    channel = grpc.insecure_channel(<span class="string">'&#123;host&#125;:&#123;port&#125;'</span>.format(host=host, port=port))</span><br><span class="line">    stub = prediction_service_pb2.PredictionServiceStub(channel)</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="comment"># Call classification model to make prediction</span></span><br><span class="line">    request = predict_pb2.PredictRequest()</span><br><span class="line">    request.model_spec.name = model</span><br><span class="line">    request.model_spec.signature_name = signature_name</span><br><span class="line"></span><br><span class="line">    feature = &#123;&#125;</span><br><span class="line">    feature[<span class="string">'a'</span>] = tf.train.Feature(float_list=tf.train.FloatList(value=[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]))</span><br><span class="line">    feature[<span class="string">'b'</span>] = tf.train.Feature(float_list=tf.train.FloatList(value=[[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]]]))</span><br><span class="line">    example = tf.train.Example(</span><br><span class="line">        features=tf.train.Features(</span><br><span class="line">            feature=feature</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    request.inputs[<span class="string">'examples'</span>].CopyFrom(make_tensor_proto([example.SerializeToString()], shape=[<span class="number">1</span>]))</span><br><span class="line">    result = stub.Predict(request, <span class="number">10.0</span>)  <span class="comment"># 10 secs timeout</span></span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    time_diff = end - start</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reference:</span></span><br><span class="line">    <span class="comment"># How to access nested values</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/44785847/how-to-retrieve-float-val-from-a-predictresponse-object</span></span><br><span class="line">    <span class="comment"># print(result)</span></span><br><span class="line">    result = result.outputs[<span class="string">'predict'</span>].float_val</span><br><span class="line">    print(result)</span><br><span class="line">    print(<span class="string">'predict shape &#123;&#125;'</span>.format(len(result)))</span><br><span class="line">    print(<span class="string">'time elapased: &#123;&#125;'</span>.format(time_diff))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, help=<span class="string">'Tensorflow server host name'</span>, default=<span class="string">'10.8.8.71'</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--port'</span>, help=<span class="string">'Tensorflow server port number'</span>, default=<span class="number">8500</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--model'</span>, help=<span class="string">'model name'</span>, default=<span class="string">'dkt'</span>, type=str)</span><br><span class="line">    parser.add_argument(<span class="string">'--signature_name'</span>, help=<span class="string">'Signature name of saved TF model'</span>,</span><br><span class="line">                        default=<span class="string">'serving_default'</span>, type=str)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    run(args.host, args.port, args.model, args.signature_name)</span><br></pre></td></tr></table></figure>
<h2 id="Freeze-pb"><a href="#Freeze-pb" class="headerlink" title="Freeze pb"></a>Freeze pb</h2><p>当不再需要改变变量，只要常量化当模型时，我们可以采用freeze pb的方式。可以用在不同语言部署的场景下，好处是除了可以冻结模型外，还可以指定剔除某些多余的节点。</p>
<h3 id="冻结"><a href="#冻结" class="headerlink" title="冻结"></a>冻结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">input_checkpoint = <span class="string">'./saved_model'</span></span><br><span class="line">output_graph = <span class="string">'./saved_model/pb/4/saved_model.pb'</span></span><br><span class="line"><span class="comment"># 指定输出的节点名称,该节点名称必须是原模型中存在的节点</span></span><br><span class="line">output_node_names = <span class="string">"Add_1"</span></span><br><span class="line"><span class="comment"># saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()  <span class="comment"># 获得默认的图</span></span><br><span class="line"><span class="keyword">with</span> graph.as_default() <span class="keyword">as</span> g:</span><br><span class="line">    a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">    b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">    w = tf.get_variable(name=<span class="string">'w'</span>, shape=[<span class="number">2</span>, <span class="number">2</span>], initializer=tf.ones_initializer())</span><br><span class="line">    c = tf.add(tf.add(a, b), w)</span><br><span class="line">    <span class="comment"># Define saver &amp; Load checkpoint</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(input_checkpoint)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        print(<span class="string">'restore True...'</span>)</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path)  <span class="comment"># 恢复图并得到数据</span></span><br><span class="line">    <span class="comment"># for op in graph.get_operations():</span></span><br><span class="line">    <span class="comment">#     print(op.name, op.values())</span></span><br><span class="line">    input_graph_def = graph.as_graph_def()  <span class="comment"># 返回一个序列化的图代表当前的图</span></span><br><span class="line">    output_graph_def = tf.graph_util.convert_variables_to_constants(  <span class="comment"># 模型持久化，将变量值固定</span></span><br><span class="line">        sess=sess,</span><br><span class="line">        input_graph_def=input_graph_def,  <span class="comment"># 等于:sess.graph_def</span></span><br><span class="line">        output_node_names=output_node_names.split(<span class="string">","</span>))  <span class="comment"># 如果有多个输出节点，以逗号隔开</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(output_graph, <span class="string">"wb"</span>) <span class="keyword">as</span> f:  <span class="comment"># 保存模型</span></span><br><span class="line">        f.write(output_graph_def.SerializeToString())  <span class="comment"># 序列化输出</span></span><br><span class="line">    print(<span class="string">"%d ops in the final graph."</span> % len(output_graph_def.node))  <span class="comment"># 得到当前图有几个操作节点</span></span><br></pre></td></tr></table></figure>
<p>运行上述程序，可在./saved_model/pb/4/下看到saved_model.pb文件。</p>
<h3 id="加载pb"><a href="#加载pb" class="headerlink" title="加载pb"></a>加载pb</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">dir = <span class="string">'./saved_model/pb/4/saved_model.pb'</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">"load graph"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.FastGFile(dir, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line">    sess.graph.as_default()</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">''</span>)</span><br><span class="line">    return_elements = [<span class="string">u'a:0'</span>, <span class="string">u'b:0'</span>, <span class="string">u'Add_1:0'</span>]</span><br><span class="line">    return_elements = tf.import_graph_def(graph_def,</span><br><span class="line">                                            return_elements=return_elements)</span><br><span class="line">    a, b, c = return_elements[<span class="number">0</span>], return_elements[<span class="number">1</span>], return_elements[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    feed_dict_testing = &#123;a: [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],</span><br><span class="line">                            b: [[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">    result = sess.run(c, feed_dict=feed_dict_testing)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>
<p>以上就是常见的TF的模型保存及对应的加载方法～</p>
<h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><ol>
<li><a href="https://ggaaooppeenngg.github.io/zh-CN/2019/03/03/tf-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%86%BB%E7%BB%93/" target="_blank" rel="noopener">tensorflow 模型的存档、保存、冻结、优化</a></li>
<li><a href="https://medium.com/@yuu.ishikawa/introduction-to-restful-api-with-tensorflow-serving-9c60969b5b95" target="_blank" rel="noopener">Introduction to RESTful API with Tensorflow Serving</a></li>
<li><a href="https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_premodeled_estimator.py" target="_blank" rel="noopener">tensorflow-serving-example</a></li>
<li><a href="https://www.tensorflow.org/guide/saved_model?hl=zh-cn" target="_blank" rel="noopener">tensorflow 官方文档</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/14/word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/word2vec/" itemprop="url">word2vec</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T12:30:39+08:00">
                2019-08-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-20T17:02:47+08:00">
                2019-08-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">Deeplearning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/NN/" itemprop="url" rel="index">
                    <span itemprop="name">NN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  0
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/13/doc2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/13/doc2vec/" itemprop="url">doc2vec</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-13T18:55:31+08:00">
                2019-08-13
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-09-20T15:32:29+08:00">
                2019-09-20
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">Deeplearning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  950
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Doc2vec"><a href="#Doc2vec" class="headerlink" title="Doc2vec"></a>Doc2vec</h1><p>常用于短文本向量化的方法包括：</p>
<ul>
<li>Bag of words</li>
<li>LDA</li>
<li>Average word vectors</li>
<li>tfidf-weighting word vector</li>
</ul>
<p>上述方法存在公共的问题是没有考虑单词的顺序。而本文介绍的doc2vec是2014年谷歌的两位大牛<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Quoc Le 和 Tomas Mikolov</a>提出的。文章提出一种无监督学习模型通过预测句子/段落中word来得到句子/段落/文档的向量表示。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>文章提出的得到句子/段落/文档向量的方法是受到word2vec的启发，因此先回顾一下word2vec。如下图所示，word2vec是通过给定单词预测另一个单词，如CBOW根据前后词预测中间词，以及Skip-gram根据中间词预测前后词。<br><img src="/2019/08/13/doc2vec/1.jpg" alt="1"><br>在模型中，每个词都用唯一的向量表示，是通过一个W矩阵通过index索引得到的每个词的向量表示，通过concat/average所有词的向量表示作为feature去预测另外一个词。在CBOW中，通过前后词预测中间词，模型最终优化的是最大化给定前后词的条件下当前词出现的概率，即：<center>$\frac{1}{T} \sum_{t=k}^{T-k} \log p\left(w_{t} | w_{t-k}, \ldots, w_{t+k}\right)$</center></p>
<p>最终模型通过softmax得到每个词出现的概率：<center>$p\left(w_{t} | w_{t-k}, \ldots, w_{t+k}\right)=\frac{e^{y_{w_{t}}}}{\sum_{i} e^{y_{i}}}$</center></p>
<p>上式中的$\boldsymbol{y}_{i}$是未归一化的每个词出现的概率：<center>$y=b+U h\left(w_{t-k}, \dots, w_{t+k} ; W\right)\qquad ...(1)$</center><br>原始论文中还提到使用层次softmax代替softmax来提升训练速度。详细细节见<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">原文</a>.</p>
<h3 id="Paragraph-Vector：-A-distributed-memory-model"><a href="#Paragraph-Vector：-A-distributed-memory-model" class="headerlink" title="Paragraph Vector： A distributed memory model"></a>Paragraph Vector： A distributed memory model</h3><p>受word2vec的启发，Paragraph Vector也是通过给出上下文预测下一个word来进行学习的，对应的框架如下：<br><img src="/2019/08/13/doc2vec/2.jpg" alt="2"><br>每个段落/句子通过矩阵D映射到向量空间中，用D的一列代替，同样，每个单词也被映射到向量空间，用矩阵W的一列表示，然后通过段落/句子向量和词向量级连或求平均得到特征预测下一个单词。与word2vec唯一不同的是在计算$y_{i}$时的h是通过W和D average/concat 得到的。</p>
<p>对于D我们可以理解为其他的word，它相当于是上下文的记忆单元活着这个段落的主题，因此我们叫这种训练方法为Distributed Memory Model of Paragraph Vector（PV-DM）。在训练时，通常采用固定长度的滑动窗口得到训练集，段落/句子向量在上下文中是共享的。</p>
<p>总结doc2vec的过程主要是两步：</p>
<ul>
<li>训练阶段，在已知数据集上训练得到模型参数D，W，U，b</li>
<li>预测阶段，得到未知段落的向量D即在固定W，U，b的情况下利用上述方法进行梯度下降，得到新的D（D中会加入表征新段落的column）</li>
</ul>
<p>优点：</p>
<ul>
<li>使用无监督学习，不需要大量有标记数据</li>
<li>能够解决bag-of-words模型的缺点：<ul>
<li>能够学到词之间的语义信息</li>
<li>考虑到了词之间的顺序</li>
</ul>
</li>
</ul>
<h3 id="Paragraph-Vector-without-worf-ordering：-Distributed-bag-of-words"><a href="#Paragraph-Vector-without-worf-ordering：-Distributed-bag-of-words" class="headerlink" title="Paragraph Vector without worf ordering： Distributed bag of words"></a>Paragraph Vector without worf ordering： Distributed bag of words</h3><p>上面提到的训练方法是要综合paragraph vector和word vector去预测下一个词，另一种训练方法可以忽略词的上下文来预测随机从段落/句子采样得到的一个词，具体来说就是每次迭代训练时，采样一个窗口的文本，然后从窗口文本中随机选一个词做预测。我们称这种方法为Distributed Bag of Words version of Paragraph Vector（PV-DBOW）。模型架构如下：<br><img src="/2019/08/13/doc2vec/3.jpg" alt="3"></p>
<hr>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h1><ol>
<li><a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a></li>
<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/" itemprop="url">Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-08T15:32:18+08:00">
                2019-08-08
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-13T19:15:28+08:00">
                2019-08-13
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">Deeplearning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/LSTM/" itemprop="url" rel="index">
                    <span itemprop="name">LSTM</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/LSTM/Embedding/" itemprop="url" rel="index">
                    <span itemprop="name">Embedding</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  1.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="EKT-Exercise-aware-Knowledge-Tracing-for-Student-Performance-Prediction"><a href="#EKT-Exercise-aware-Knowledge-Tracing-for-Student-Performance-Prediction" class="headerlink" title="EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction"></a>EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</h2><p>这篇[文章][10]很早就看到过，但是到2019年6月份才发表出来，本文章认为目前的知识追踪的model都只用了学生的做题信息，而其他的如知识概念或习题内容相关的知识并没有在model中引入，而引入这些信息是能够为模型的预测精度带来增益的，因此作者提出Exercise-Enhanced Recurrent Neural Network(EERNN)，在这个模型中，不仅使用了学生的做题记录，还引入了习题的文本信息。在EERNN中，作者使用RNN的隐变量来表征学生的学习轨迹，使用BiLSTM来学习习题的编码信息。在最终predict阶段，作者在EERNN基础上采用了两种策略，一种是EERNNM with Markov property,另一种是EERNNA with attention mechanism。最终为了追踪学生在各知识点上的掌握情况，作者将EERNN引入知识概念的信息引入得到Exercise-Aware Knowledge Tracing（EKT）。</p>
<h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><p>模型主要分两部分来描述，一是做预测的EERNN（EERNNM &amp; EERNNA），另一部分是追踪学生知识掌握情况的EKT。</p>
<h4 id="EERNN"><a href="#EERNN" class="headerlink" title="EERNN"></a>EERNN</h4><p>EERNN的提出主要用于做学生performance的预测，基于不同的策略又分为EERNNM和EERNNA。网络结果如下图所示：</p>
<p><img src="/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/EERNN.jpg" alt="EERNN"></p>
<p>从上图可以看出：EERNNM和EERNNA的区别主要在prediction阶段，图中橘色框表示的是题目的embeddig，蓝色框表示的是学生的embedding。</p>
<h5 id="Exercise-embedding"><a href="#Exercise-embedding" class="headerlink" title="Exercise embedding:"></a>Exercise embedding:</h5><p>Exercise embedding的获取是通过双向LSTM得到的，结构如下：<br><img src="/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/EERNN-1.jpg" alt="EERNN"></p>
<p>通过上述可以得到每个word的embedding $v_{m}=$ concatenate $\left(\vec{v}_{m}, \overleftarrow v_{m}\right)$，最终Exercise的embedding是通过max-pooling每个word的embedding得到，即$x_{i}=\max \left(v_{1}, v_{2}, \ldots, v_{M}\right) x_{i} \in \mathbb{R}^{2 d_{v}}$。</p>
<h5 id="Student-Embedding："><a href="#Student-Embedding：" class="headerlink" title="Student Embedding："></a>Student Embedding：</h5><p>表征学生的向量应该跟题目和学生的回答有关，因此作者在上面获得$x_{i}$的基础上加入了表示学生作答情况的信息，通过RNN/LSTM得到student embedding，具体实现是首先将$r_{t}$表示成一个$2 d_{v}$维的$\mathbf{0}=(0,0, \ldots, 0)$，最终输入$\widetilde{x}_{t} \in \mathbb{R}^{4 d_{v}}$表示为：<center>$\widetilde{x}_{t}=\left\{\begin{array}{ll}{\left[x_{t} \oplus \mathbf{0}\right]} & {\text { if } r_{t}=1} \\ {\left[\mathbf{0} \oplus x_{t}\right]} & {\text { if } \quad r_{t}=0}\end{array}\right.$</center></p>
<h5 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h5><ul>
<li><p>EERNNM:</p>
<p>基于markov性，下一时刻状态的条件概率分布只与当前状态有关，因此对$\widetilde{r}_{T+1}$的预测只与$h_{T}$和$x_{T+1}$有关，因此计算公式如下：<center>$\begin{aligned} y_{T+1} &=\operatorname{Re} L U\left(\mathbf{W}_{1} \cdot\left[h_{T} \oplus x_{T+1}\right]+\mathbf{b}_{1}\right) \\ \widetilde{r}_{T+1} &=\sigma\left(\mathbf{W}_{2} \cdot y_{T+1}+\mathbf{b}_{2}\right) \end{aligned}$   ... (1)</center></p>
</li>
<li><p>EERNNA<br>如果序列很长的话，LSTM捕捉信息的能力会降低，因此为了改善上述问题，引入常用的Attention机制。<br>经过attention后的表征当前状态的隐变量变为：<center>$h_{a t t}=\sum_{j=1}^{T} \alpha_{j} h_{j} \\ \alpha_{j}=\cos \left(x_{T+1}, x_{j}\right)$</center><br>将(1)式中的$h_{a t t}$替换$h_{T}$即可得到预测结果。</p>
</li>
</ul>
<h4 id="EKT-Exercise-aware-Knowledge-Tracing"><a href="#EKT-Exercise-aware-Knowledge-Tracing" class="headerlink" title="EKT: Exercise-aware Knowledge Tracing"></a>EKT: Exercise-aware Knowledge Tracing</h4><p>EKT本质做的是将原始EERNN学习到的学生状态从$h_{t} \in \mathbb{R}^{d_{h}}$变换成$\dot{H}_{t} \in \mathbb{R}^{\dot{d}_{h} \times K}$，也就是说用一个向量来表征学生对某个知识的掌握情况。模型结构如下图所示：<br><img src="/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/EKT.jpg" alt="EKT"><br>与EERNN相比，除了使用到Exercise Embedding还用到了Knowledge Embedding（对应图中绿色的部分）。</p>
<h5 id="Knowledge-Embedding："><a href="#Knowledge-Embedding：" class="headerlink" title="Knowledge Embedding："></a>Knowledge Embedding：</h5><p>由于知识之间是相关的而非独立的，因此作者引入了memory module来计算当前的知识点与其他知识点的相关性，并最终影响到学生知识状态的隐变量，其中知识间的相关性是通过$\beta_{t}^{i}$实现的。如图中标注，k（K维，K表示所有知识点）表示当前时刻题目对应的知识点的one-hot编码，v（$d_{k}$维）则是将k进行地位压缩后的编码向量$v_{t}=\mathbf{W}_{\mathbf{k}}^{\mathrm{T}} k_{t}$，通过memory module（本质上是一个$d_{k} \times  K$的矩阵），最终$\beta_{t}^{i}$计算公式如下：<center>$\beta_{t}^{i}=\operatorname{Softmax}\left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)=\frac{\exp \left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)}{\sum_{i=1}^{K}\left(\exp \left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)\right)}$</center><br>最终隐状态表示为：<br>$H_{t}^{i}=L S T M\left(\widetilde{x}_{t}^{i}, H_{t-1}^{i} ; \theta_{H^{i}}\right)$，<br>其中$\widetilde{x}_{t}^{i}=\beta_{t}^{i} \hat{x}_{t}$。</p>
<h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><p><img src="/2019/08/08/Exercise-Enhanced-Sequential-Modeling-for-Student-Performance-Prediction/EKT-2.jpg" alt="EKT"></p>
<h3 id="思考："><a href="#思考：" class="headerlink" title="思考："></a>思考：</h3><ul>
<li>EERNN模型为什么只引入题目信息和学生做题记录，为什么不引入知识结构信息？</li>
<li>EKT和EERNN本质是可以做相同的事情，并且结果表明EKT也由于EERNN相关模型。</li>
<li>对应Exercise Embedding为什么采用预训练而不端到端统一到一个model中？</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h2><ol>
<li><a href="https://arxiv.org/pdf/1906.05658.pdf" target="_blank" rel="noopener">EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/08/阿里云 cuda9.1+cudnn 7.1+pytorch 环境搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/08/阿里云 cuda9.1+cudnn 7.1+pytorch 环境搭建/" itemprop="url">阿里云 cuda9.1+cudnn 7.1+pytorch 环境搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-08T15:32:00+08:00">
                2019-08-08
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-08T15:28:33+08:00">
                2019-08-08
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/环境搭建/" itemprop="url" rel="index">
                    <span itemprop="name">环境搭建</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  269
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-阿里云ECS-默认安装cuda-9-1版本"><a href="#1-阿里云ECS-默认安装cuda-9-1版本" class="headerlink" title="1.阿里云ECS 默认安装cuda 9.1版本"></a>1.阿里云ECS 默认安装cuda 9.1版本</h3><ul>
<li>确定cuda版本： <ul>
<li>首先使用nvidia -smi 查看显卡驱动运行状态</li>
<li>使用nvcc -V查看cuda-toolkit安装是否成功<ul>
<li>若显示nvcc命令不存在：<ul>
<li>使用whereis cuda 查看cuda路径</li>
<li>在～/.bashrc 中加入：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin:$PATH </span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64/</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>source ～/.bashrc 命令使环境变量生效    </p>
<h3 id="2-安装Anaconda："><a href="#2-安装Anaconda：" class="headerlink" title="2.  安装Anaconda："></a>2.  安装Anaconda：</h3><p> <a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a>   版本 Anaconda3-5.1.0-Linux-x86_64.sh</p>
<h3 id="3-安装cudnn："><a href="#3-安装cudnn：" class="headerlink" title="3. 安装cudnn："></a>3. 安装cudnn：</h3><p>从官网下载 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a><br>解压tar -xzvf cudnn-9.1-linux-x64-v7.1.tgz<br>copy到对应cuda文件夹下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p>
<p>查看cudnn版本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A5</span><br></pre></td></tr></table></figure></p>
<h3 id="4-安装pytorch"><a href="#4-安装pytorch" class="headerlink" title="4. 安装pytorch"></a>4. 安装pytorch</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cuda91 -c pytorch</span><br></pre></td></tr></table></figure>
<p>⚠️： 若不成功，可以查看pytorch 版本，可以更改pytorch版本：(实测可用)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==0.4.1</span><br><span class="line">pip install torchvision==0.2.2</span><br></pre></td></tr></table></figure></p>
<h3 id="5-检查安装是否成功："><a href="#5-检查安装是否成功：" class="headerlink" title="5. 检查安装是否成功："></a>5. 检查安装是否成功：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line">import torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>
<p>【参考文章】：</p>
<ol>
<li><a href="https://blog.csdn.net/qq_29762941/article/details/80630585" target="_blank" rel="noopener">https://blog.csdn.net/qq_29762941/article/details/80630585</a></li>
<li><a href="https://blog.csdn.net/qq_29762941/article/details/80630585" target="_blank" rel="noopener">https://blog.csdn.net/qq_29762941/article/details/80630585</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/06/深度知识追踪/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/06/深度知识追踪/" itemprop="url">深度知识追踪</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-06T18:32:00+08:00">
                2019-08-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-13T19:09:17+08:00">
                2019-08-13
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/" itemprop="url" rel="index">
                    <span itemprop="name">Deeplearning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/DKT/" itemprop="url" rel="index">
                    <span itemprop="name">DKT</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deeplearning/DKT/TCN/" itemprop="url" rel="index">
                    <span itemprop="name">TCN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  5.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  21
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="深度知识追踪（Deep-Knowledge-Tracing）"><a href="#深度知识追踪（Deep-Knowledge-Tracing）" class="headerlink" title="深度知识追踪（Deep Knowledge Tracing）"></a>深度知识追踪（Deep Knowledge Tracing）</h1><p>知识追踪是基于学生行为序列进行建模，预测学生对知识的掌握程度。知识追踪是构建自适应教育系统的核心和关键。在自适应的教育系统中，无论是做精准推送，学生学习的路径规划或知识图谱的构建，第一步都是能够精准预测学生对知识的掌握程度。</p>
<p>知识追踪问题可以描述为： 给定一学生的观测序列 $x_{0},\ldots\ldots,x_{t}$ 预测下次表现$x_{t+1}$，通常$\mathbf { x } _ { t } = \left\{ q _ { t } , a _ { t } \right\}$，其中$q _ { t }$代表回答的问题成分（如对应的知识点），$a _ { t }$代表对应的回答是否正确，通常$a _ { t }=\left\{0,1\right\}$。 下图描述了一个学生在八年级数学中的知识追踪结果可视化展示。<br><img src="/2019/08/06/深度知识追踪/1.png" alt="1"><br>传统的知识追踪是基于一阶马尔可夫模型，如<a href="https://www.cs.cmu.edu/~ggordon/yudelson-koedinger-gordon-individualized-bayesian-knowledge-tracing.pdf" target="_blank" rel="noopener">贝叶斯知识追踪</a>(Bayesian Knowledge Tracing)，2015年 <a href="https://arxiv.org/pdf/1506.05908.pdf" target="_blank" rel="noopener">Chris Piech</a> 等人提出利用深度学习来处理知识追踪的任务，之后引发了学者们对利用深度学习来处理知识追踪任务对不断探讨，下面依次介绍相关论文核心内容及思考。</p>
<h2 id="Deep-Knowledge-Tracing"><a href="#Deep-Knowledge-Tracing" class="headerlink" title="Deep Knowledge Tracing"></a>Deep Knowledge Tracing</h2><p>循环神经网络（RNN）是一种时间序列的模型，天然具有高维连续的隐状态表示的特征，RNN能够利用早期的信息进行预测，因此在2015年 <a href="https://arxiv.org/pdf/1506.05908.pdf" target="_blank" rel="noopener">Chris Piech</a> 将RNN应用于知识追踪领域，并取得了较好的结果。</p>
<h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3><p>文章中，作者采用了传统的RNN模型和其变种LSTM，其输入数据为经过编码的$q _ { t }$，输出为$a _ { t }$。对于输入的编码方式有两种：</p>
<ol>
<li>将输入进行one-hot编码，如模型输入数据涉及M个知识成分（如知识点），每道题有两种结果0，1（分别对应答错和答对），则模型输入长度为2M。例如，对于某题，其知识成分为i，若答对，对应输入为第i+1位为1其余位置为0；若答错，则第i位为1其余位置为0。</li>
<li>将输入进行压缩，若知识成分M巨大时，输入维度过高，可以采用感知压缩使输入从2M降低至$\log (2 M)$。</li>
</ol>
<p>模型输出$y _ { t }$ 长度为M，对应描述了每一个知识成分的掌握程度（即对应知识成分所对应题目的答对概率），模型的核心为使用前$t$时刻的学生做题序列预测$t+1$时刻的知识成分的掌握情况，即：$P(y_{t+1}|x_{t} ,  \ldots \ldots , x _ { t })$。<br>模型的目标函数是观测序列的非负对数似然函数，假设 $\delta \left( q _ { t + 1 } \right)$为$t+1$时刻的编码输入，$l$为二进制交叉上函数，目标函数如下：<center> $L = \sum _ { t } \ell \left( \mathbf { y } _ { i } ^ { T } \delta \left( q _ { t + 1 } \right) , a _ { t + 1 } \right)$  </center> </p>
<h3 id="模型优缺点："><a href="#模型优缺点：" class="headerlink" title="模型优缺点："></a>模型优缺点：</h3><h5 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h5><ol>
<li>能够反应长时间的知识关系，基于RNN的特性能够根据学生近期学习表现进行预测（<a href="https://arxiv.org/pdf/1604.02416.pdf" target="_blank" rel="noopener">近因效应</a>），也能根据实际学生学习路径进行建模；</li>
<li>能够对复杂的知识点之间的联系进行建模，如构建知识图谱；</li>
<li>能够处理多知识成分的问题。<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><a href="https://arxiv.org/pdf/1806.02180.pdf" target="_blank" rel="noopener">缺点：</a></h5></li>
<li>模型无法重构输入，即输入某一知识成分答题错误，模型对该知识成分的预测反而是正确。</li>
<li>在时间序列上，学生对知识点的掌握程度不具有连续一致性，波动情况较大。</li>
</ol>
<h2 id="Going-Deeper-with-Deep-Knowledge-Tracing"><a href="#Going-Deeper-with-Deep-Knowledge-Tracing" class="headerlink" title="Going Deeper with Deep Knowledge Tracing"></a>Going Deeper with Deep Knowledge Tracing</h2><p>2016年<a href="http://www.educationaldatamining.org/EDM2016/proceedings/paper_133.pdf" target="_blank" rel="noopener">Xiaolu Xiong</a>等人对DKT和PFA(Performance Factor Analysis)，BKT(Bayesian Knowledge Tracing)模型进行了比较，对DKT模型能碾压其他两种模型的结果进行了怀疑并加以论证，进一步讨论了原论文能够得出上述结果的原因，对进一步使用DKT模型提供了参考。<br>文章中指出，对于DKT文章中在ASSISTments数据集上取得好的结果的原因进行了分析，得到以下三个原因来说明为什么DKT能够取得碾压BKT的效果：</p>
<ol>
<li>ASSISTments数据集中存在23.6%的重复数据，这部分数据应该舍弃而不应该用于训练或测试。</li>
<li>DKT模型在实验中，并没有去除脚手架式的教学问题的做题记录，这就导致DKT模型能够有更多信息引入模型。</li>
<li>由于DKT处理多知识成分的问题时，单条做题记录会被扩展成多条，存在重复利用数据的问题。</li>
</ol>
<p>作者将上述问题数据依次剔除，形成多个数据集，并采用ACU和$r ^ { 2 }$作指标，对DKT，PFA，BKT模型进行了对比，<strong>结果表明DKT相比BKT和PFA没有碾压式的超越但是的确会比其他模型结果要好</strong>。<br>文章中提及，当遇到多知识成分的题目时，使用联合知识成分作为新的知识成分的方式比重复利用做题记录的方式结果要差很多，这也是在我们实际使用DKT模型中需要注意的。</p>
<h2 id="Addressing-Two-Problems-in-Deep-Knowledge-Tracing-viaPrediction-Consistent-Regularization"><a href="#Addressing-Two-Problems-in-Deep-Knowledge-Tracing-viaPrediction-Consistent-Regularization" class="headerlink" title="Addressing Two Problems in Deep Knowledge Tracing viaPrediction-Consistent Regularization"></a>Addressing Two Problems in Deep Knowledge Tracing viaPrediction-Consistent Regularization</h2><p><a href="https://arxiv.org/pdf/1806.02180.pdf" target="_blank" rel="noopener">Chun-Kit Yeung</a>等人在2018年6月发表论文中指出DKT模型现存的缺点，<strong><em>即对输入序列存在重构问题和预测结果的波动性</em></strong>，进而论文提出了改善上述问题的方法：增加对应的正则项，得到<strong>DKT+</strong>（增强的DKT模型）。</p>
<p><img src="/2019/08/06/深度知识追踪/2.png" alt="2"></p>
<p>如上图所示，纵轴$S _ { i }$表示知识成分，横轴为学生在各个知识成分上的答题情况。<br>问题1对应的是在$6 ^ { t h }$step，$S _ { 45 }$的评估结果相比前一时刻是增加了的即使当前的输入是$S _ { 45 }$做错了的。<br>问题2对应为在上述图描述的学习过程中，$S _ { 32 } , S _ { 33 } , S _ { 45 }$和$S _ { 55 }$预测做对的概率随着 $S _ { 32 } , S _ { 33 }$的学习有着非常大的波动，这和我们实际情况是不相符的，我们总是期望知识成分的变化是随着时间缓慢变化的，而不是在掌握和没掌握之间跳跃。</p>
<p>对于问题1，作者认为出现这种情况是由于，在DKT模型采用的损失函数中并没有考虑到时间t时刻的输入值，只是考虑了t时刻的输出值和t+1时刻的输入值。为了解决上述问题，作者在损失函数中引入正则项，并在正则项中引入了时间t时刻的输入值，正则项r如下：<center>$r = \frac { 1 } { \sum _ { i = 1 } ^ { n } \left( T _ { i } - 1 \right) } \left( \sum _ { i = 1 } ^ { n } \sum _ { t = 1 } ^ { T _ { i } - 1 } l \left( \mathbf { y } _ { t } ^ { i } \cdot \boldsymbol { \delta } \left( q _ { t } ^ { i } \right) , a _ { t } ^ { i } \right) \right)$</center></p>
<p>对于问题2，作者认为可能是由于RNN的隐层表示问题，RNN的隐含层$\mathbf { h } _ { t }$依赖于前一隐含层的输出$\mathbf { h } _ { t -1}$和当前输入$\mathbf { x } _ { t }$，隐含层表示了潜在的学生知识掌握程度，但是很难说清隐含层的每个状态是如何影响对知识成分的预测。所以简答起见，直接对输出结果进行正则约束（L1，L2正则），使预测结果能够平滑输出，所加正则项如下：<center>$w _ { 1 } = \frac { \sum _ { i = 1 } ^ { n } \sum _ { t = 1 } ^ { T _ { i } - 1 } \left\| \mathbf { y } _ { t + 1 } ^ { i } - \mathbf { y } _ { t } ^ { i } \right\| _ { 1 } } { M \sum _ { i = 1 } ^ { n } \left( T _ { i } - 1 \right) }$</center><center>$w _ { 2 } ^ { 2 } = \frac { \sum _ { i = 1 } ^ { n } \sum _ { t = 1 } ^ { T _ { i } - 1 } \left\| \mathbf { y } _ { t + 1 } ^ { i } - \mathbf { y } _ { t } ^ { i } \right\| _ { 2 } ^ { 2 } } { M \sum _ { i = 1 } ^ { n } \left( T _ { i } - 1 \right) }$</center><br>综合上述正则项，最终模型的损失函数为：<center>$\mathcal { L } ^ { \prime } = \mathcal { L } + \lambda _ { r } r + \lambda _ { w _ { 1 } } w _ { 1 } + \lambda _ { w _ { 2 } } w _ { 2 } ^ { 2 }$</center></p>
<h2 id="Does-Deep-Knowledge-Tracing-Model-Interactions-Among-Skills"><a href="#Does-Deep-Knowledge-Tracing-Model-Interactions-Among-Skills" class="headerlink" title="Does Deep Knowledge Tracing Model Interactions Among Skills?"></a>Does Deep Knowledge Tracing Model Interactions Among Skills?</h2><p><a href="http://educationaldatamining.org/files/conferences/EDM2018/papers/EDM2018_paper_208.pdf" target="_blank" rel="noopener">Shirly Montero</a> 等人在2018EDM上发表的paper，通过对比DKT和BKT的不同，来分析得到为何DKT能够获得比较好的结果的原因，论文从三个方面进行了比较：1）模型规模上，DKT是一个具有多自由参数的神经网络模型，而BKT是一个概率模型，拥有有限的自由参数。2）DKT对一个领域内所有skill建模，而BKT是根据skill构造模型。3）DKT是多skill的交叉输入，而BKT是按照skill进行拆分的。最终得到的结论是1),3)是DKT取得好的效果的关键。<br>为验证，通过如下方式区分DKT和BKT的不同模型：</p>
<ul>
<li>DKT是随着时间的推移，所有的skill都是交织在一起的，并按照顺序对每个问题做预测，而BKT是根据技能单独做成对应的序列，这种区别对应于combined sequence (CS)和 separate sequence (SS)。</li>
<li>DKT是通过一个model学习所有的skill，而BKT假设对每个skill训练单独的model，这种区别对应于combined model(CM) 和 separate model(SM)。</li>
<li>DKT基于神经网络而BKT基于概率模型。因此DKT相比BKT拥有更多的自由变量。</li>
</ul>
<p>基于上述不同，作者分别采用了四种模型在三种数据集上做了验证，来证明是哪些特征使得DKT拥有比较好的效果，结果如下：</p>
<p><img src="/2019/08/06/深度知识追踪/3.png" alt="3"><br>上述结果中BKT-SM-SS对应于标准的BKT模型，DKT-SM-SS是对不同对skill采用不同对model建模，对应的输入也是当前skill 的序列。DKT-CM-SS使用同一个model对所有skill做预测，区别于标准的DKT是输入是按照skill进行分割的。DKT-CM-CS则对应于标准的DKT模型。<br>通过上述结果我们可以得到：</p>
<ul>
<li>DKT可以通过交织的都skill序列获得增益。</li>
<li>对比DKT-SM-SS和DKT-CM-SS的结果可以看到，通过一个模型对按照skill进行分割的数据做预测并不能得到增益。</li>
<li>DKT并没有像BKT一样引入强人类学习理论，这或许可以解释简单的全有或全无的学习——没有遗忘的BKT假设的理论过于简单。</li>
</ul>
<h2 id="Incorporating-Features-Learned-by-an-Enhanced-Deep-Knowledge-Tracing-Model-for-STEM-Non-STEM-Job-Prediction"><a href="#Incorporating-Features-Learned-by-an-Enhanced-Deep-Knowledge-Tracing-Model-for-STEM-Non-STEM-Job-Prediction" class="headerlink" title="Incorporating Features Learned by an Enhanced Deep Knowledge Tracing Model for STEM/Non-STEM Job Prediction"></a>Incorporating Features Learned by an Enhanced Deep Knowledge Tracing Model for STEM/Non-STEM Job Prediction</h2><p>该模型是<a href="https://arxiv.org/pdf/1604.02416.pdf" target="_blank" rel="noopener">Chun-Kit Yeung</a>等人在参加2017 ASSISTments Data Mining 竞赛中使用的方案，该竞赛是使用中学的学生行为预测高中/大学的结果。本文章在DKT和DKT+的基础上，使用了其他学生相关的综合特征，实验结果表明使用了其他特征的效果要比基础模型好。该文章为我们提供了引入其他学生行为特征的方法。<br>本文提出的模型主要思想是利用DKT+学习序列数据，得到$\mathbf { X }_{ K T}$代表学生的最新的知识状态，然后结合学生其他特征$\mathbf { X }_{ SP}$得到feature set$\mathbf { x } _ { f } = \left[ \mathbf { x } _ { S P } , \mathbf { x } _ { K T } \right]$送入机器学习模型(如GBDT， LR，LDA， SVM等)得到最终的预测结果。<br>上述模型的思想比较简单，可以看作集成模型Stacking的一种方式，整个模型的框架如下：<br><img src="/2019/08/06/深度知识追踪/4.png" alt="4"></p>
<h2 id="Prerequisite-Driven-Deep-Knowledge-Tracing"><a href="#Prerequisite-Driven-Deep-Knowledge-Tracing" class="headerlink" title="Prerequisite-Driven Deep Knowledge Tracing"></a>Prerequisite-Driven Deep Knowledge Tracing</h2><p><a href="https://arxiv.org/pdf/1806.03256.pdf" target="_blank" rel="noopener">Penghe Chen</a>等人在使用DKT时考虑到数据的稀疏性(skill空间比较大，学生做题比较有限)，为解决由于数据稀疏性带来的模型评估不准确，提出了将知识结构的信息纳入模型来解决上述问题，具体是指考虑来知识的前后置关系。<br><strong>核心观点在于如果$K_1$是$K_2$的前置，则$K_2$的掌握程度要小于等于$K_1$，即后置的掌握程度要小于等于前置的掌握程度。</strong><br>具体过程如下：<br>如果$K_1$是$K_2$的前置，则：</p>
<ul>
<li>$P(M_{i,k_{2},t_{2}}=1)$很大，则$P(M_{i,k_{1},t_{1}}=1)$更大：<pre><code>  即学生i如果在$t_{2}$时刻掌握了$K_{2}$，则说明在前一时刻，学生已经掌握了其前置知识$K_1$；
</code></pre></li>
<li>$P(M_{i,k_{1},t_{1}}=1)$很小，则$P(M_{i,k_{2},t_{2}}=1)$更小：<pre><code>  即学生i如果在$t_{1}$时刻没有掌握$K_{1}$，则说明在下一时刻，学生更不可能掌握其后置知识$K_2$；
</code></pre></li>
</ul>
<p>上述知识结构上的信息体现在model中是通过修改损失函数的方式实现的，具体损失函数如下：<center>$\max _ { \Theta } \log \prod _ { i } \prod _ { t } P \left( y _ { i , \pi ( i , t ) , t } | \mathbf { s } _ { i } , \Theta \right)$
s.t. $P \left( m _ { i , k _ { 2 } , t _ { 2 } } = 1 \right) \leq P \left( m _ { i , k _ { 1 } , t _ { 1 } } = 1 \right)$
$\quad \forall \left( k _ { 1 } , k _ { 2 } \right) \in E \& y _ { i , \pi \left( i , t _ { 1 } \right) , t _ { 1 } } = y _ { i , \pi \left( i , t _ { 2 } \right) , t _ { 2 } }$</center><br>对上述进行变形，将强约束变为弱正则引入模型，损失函数为：<center>$\max _ { \Theta } \sum _ { i } \sum _ { t } \log P \left( y _ { i , \pi ( i , t ) , t } | \mathbf { s } _ { i } ,             \Theta \right) +$
$\lambda \sum _ { i } \sum _ { k _ { 1 } , k _ { 2 } } \sum _ { t } \sum _ { t _ { 2 } } \delta ( * ) \left[ \log P \left( m _ { i , k _ { 1 } , t _ { 1 } } \right) - \log P \left( m _ { i , k _ { 2 } , t _ { 2 } } \right) \right]$</center></p>
<p>下面分两部分来解释上面的loss：</p>
<ul>
<li>第一部分，其中$\mathbf { s } _ { i }$表示学生i的做题序列，$\Theta$表示模型的所有参数，$\pi ( i , t )$表示学生i在t时刻回答的题目，$y _ { i , \pi ( i , t ) , t }$对应学生i在t时刻回答题目的answer，因此loss的第一部分是在给定模型和序列下的结果最大化。</li>
<li>第二部分，对应于知识结构的前置约束，其中$\delta ( * ) =\delta \left( y _ { i , \pi \left( i , t _ { 1 } \right) , t _ { 1 } } = y _ { i , \pi \left( i , t _ { 2 } \right) , t _ { 2 } } \right)$，即只有前后两个时刻的answer一致且存在前后置关系时，第二部分的正则才会有效。</li>
</ul>
<p>最终实验结果表明，加入知识结构信息的model比原始DKT结果要好，具体详见paper。<br>(PS: 笔者在本公司的数据集上验证并没有得到提升😢)</p>
<h2 id="EKT-Exercise-aware-Knowledge-Tracing-for-Student-Performance-Prediction"><a href="#EKT-Exercise-aware-Knowledge-Tracing-for-Student-Performance-Prediction" class="headerlink" title="EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction"></a>EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</h2><p>这篇<a href="https://arxiv.org/pdf/1906.05658.pdf" target="_blank" rel="noopener">文章</a>很早就看到过，但是到2019年6月份才发表出来，本文章认为目前的知识追踪的model都只用了学生的做题信息，而其他的如知识概念或习题内容相关的知识并没有在model中引入，而引入这些信息是能够为模型的预测精度带来增益的，因此作者提出Exercise-Enhanced Recurrent Neural Network(EERNN)，在这个模型中，不仅使用了学生的做题记录，还引入了习题的文本信息。在EERNN中，作者使用RNN的隐变量来表征学生的学习轨迹，使用BiLSTM来学习习题的编码信息。在最终predict阶段，作者在EERNN基础上采用了两种策略，一种是EERNNM with Markov property,另一种是EERNNA with attention mechanism。最终为了追踪学生在各知识点上的掌握情况，作者将EERNN引入知识概念的信息引入得到Exercise-Aware Knowledge Tracing（EKT）。</p>
<h3 id="模型：-1"><a href="#模型：-1" class="headerlink" title="模型："></a>模型：</h3><p>模型主要分两部分来描述，一是做预测的EERNN（EERNNM &amp; EERNNA），另一部分是追踪学生知识掌握情况的EKT。</p>
<h4 id="EERNN"><a href="#EERNN" class="headerlink" title="EERNN"></a>EERNN</h4><p>EERNN的提出主要用于做学生performance的预测，基于不同的策略又分为EERNNM和EERNNA。网络结果如下图所示：</p>
<p><img src="/2019/08/06/深度知识追踪/EERNN.jpg" alt="EERNN"></p>
<p>从上图可以看出：EERNNM和EERNNA的区别主要在prediction阶段，图中橘色框表示的是题目的embeddig，蓝色框表示的是学生的embedding。</p>
<h5 id="Exercise-embedding"><a href="#Exercise-embedding" class="headerlink" title="Exercise embedding:"></a>Exercise embedding:</h5><p>Exercise embedding的获取是通过双向LSTM得到的，结构如下：<br><img src="/2019/08/06/深度知识追踪/EERNN-1.jpg" alt="EERNN"></p>
<p>通过上述可以得到每个word的embedding $v_{m}=$ concatenate $\left(\vec{v}_{m}, \overleftarrow v_{m}\right)$，最终Exercise的embedding是通过max-pooling每个word的embedding得到，即$x_{i}=\max \left(v_{1}, v_{2}, \ldots, v_{M}\right) x_{i} \in \mathbb{R}^{2 d_{v}}$。</p>
<h5 id="Student-Embedding："><a href="#Student-Embedding：" class="headerlink" title="Student Embedding："></a>Student Embedding：</h5><p>表征学生的向量应该跟题目和学生的回答有关，因此作者在上面获得$x_{i}$的基础上加入了表示学生作答情况的信息，通过RNN/LSTM得到student embedding，具体实现是首先将$r_{t}$表示成一个$2 d_{v}$维的$\mathbf{0}=(0,0, \ldots, 0)$，最终输入$\widetilde{x}_{t} \in \mathbb{R}^{4 d_{v}}$表示为：<center>$\widetilde{x}_{t}=\left\{\begin{array}{ll}{\left[x_{t} \oplus \mathbf{0}\right]} & {\text { if } r_{t}=1} \\ {\left[\mathbf{0} \oplus x_{t}\right]} & {\text { if } \quad r_{t}=0}\end{array}\right.$</center></p>
<h5 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h5><ul>
<li><p>EERNNM:</p>
<p>基于markov性，下一时刻状态的条件概率分布只与当前状态有关，因此对$\widetilde{r}_{T+1}$的预测只与$h_{T}$和$x_{T+1}$有关，因此计算公式如下：<center>$\begin{aligned} y_{T+1} &=\operatorname{Re} L U\left(\mathbf{W}_{1} \cdot\left[h_{T} \oplus x_{T+1}\right]+\mathbf{b}_{1}\right) \\ \widetilde{r}_{T+1} &=\sigma\left(\mathbf{W}_{2} \cdot y_{T+1}+\mathbf{b}_{2}\right) \end{aligned}$   ... (1)</center></p>
</li>
<li><p>EERNNA<br>如果序列很长的话，LSTM捕捉信息的能力会降低，因此为了改善上述问题，引入常用的Attention机制。<br>经过attention后的表征当前状态的隐变量变为：<center>$h_{a t t}=\sum_{j=1}^{T} \alpha_{j} h_{j} \\ \alpha_{j}=\cos \left(x_{T+1}, x_{j}\right)$</center><br>将(1)式中的$h_{a t t}$替换$h_{T}$即可得到预测结果。</p>
</li>
</ul>
<h4 id="EKT-Exercise-aware-Knowledge-Tracing"><a href="#EKT-Exercise-aware-Knowledge-Tracing" class="headerlink" title="EKT: Exercise-aware Knowledge Tracing"></a>EKT: Exercise-aware Knowledge Tracing</h4><p>EKT本质做的是将原始EERNN学习到的学生状态从$h_{t} \in \mathbb{R}^{d_{h}}$变换成$\dot{H}_{t} \in \mathbb{R}^{\dot{d}_{h} \times K}$，也就是说用一个向量来表征学生对某个知识的掌握情况。模型结构如下图所示：<br><img src="/2019/08/06/深度知识追踪/EKT.jpg" alt="EKT"><br>与EERNN相比，除了使用到Exercise Embedding还用到了Knowledge Embedding（对应图中绿色的部分）。</p>
<h5 id="Knowledge-Embedding："><a href="#Knowledge-Embedding：" class="headerlink" title="Knowledge Embedding："></a>Knowledge Embedding：</h5><p>由于知识之间是相关的而非独立的，因此作者引入了memory module来计算当前的知识点与其他知识点的相关性，并最终影响到学生知识状态的隐变量，其中知识间的相关性是通过$\beta_{t}^{i}$实现的。如图中标注，k（K维，K表示所有知识点）表示当前时刻题目对应的知识点的one-hot编码，v（$d_{k}$维）则是将k进行地位压缩后的编码向量$v_{t}=\mathbf{W}_{\mathbf{k}}^{\mathrm{T}} k_{t}$，通过memory module（本质上是一个$d_{k} \times  K$的矩阵），最终$\beta_{t}^{i}$计算公式如下：<center>$\beta_{t}^{i}=\operatorname{Softmax}\left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)=\frac{\exp \left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)}{\sum_{i=1}^{K}\left(\exp \left(v_{t}^{\mathrm{T}} \mathbf{M}_{i}\right)\right)}$</center><br>最终隐状态表示为：<br>$H_{t}^{i}=L S T M\left(\widetilde{x}_{t}^{i}, H_{t-1}^{i} ; \theta_{H^{i}}\right)$，<br>其中$\widetilde{x}_{t}^{i}=\beta_{t}^{i} \hat{x}_{t}$。</p>
<h2 id="A-Self-Attentive-model-for-Knowledge-Tracing"><a href="#A-Self-Attentive-model-for-Knowledge-Tracing" class="headerlink" title="A Self-Attentive model for Knowledge Tracing"></a>A Self-Attentive model for Knowledge Tracing</h2><p>该模型是<a href="https://arxiv.org/pdf/1907.06837.pdf" target="_blank" rel="noopener">Shalini Pandey</a>等人在2019年7月post出来的论文，核心就是将Transformer引入只是追踪领域代替LSTM/RNN。该论文将Transformer引入后的model称为SAKT，模型架构如下：</p>
<p> <img src="/2019/08/06/深度知识追踪/SAKT.jpg" alt="SAKT"></p>
<p>模型的核心架构跟Transformer一致（对于Transformer的结构和理解，这里不多讲，请自行学习），都主要包括multi-head attention (对应文章的self-attention layer)，feed forward layer, residual connections, layer normalization 和prediction layer。重点的区别在于attention layer的输入embedding上。Transormer的输入是input经过embedding再加上Position Encoding，而本文章的输入有两部分， 一部分是用户reation的，即用户做了哪些题目以及做对做错$\mathbf{x}_{t}=\left(e_{t}, r_{t}\right)$,其中$e_{t}$表示t时刻学生做的题目，$r_{t}$表示学生t时刻的做题正误； 另一部分仅仅是题目即$e_{t}$，作者用$x_{i}$的embedding作为key和value，而$e_{t}$作为query，具体如下图所示：</p>
<p> <img src="/2019/08/06/深度知识追踪/SAKT-1.jpg" alt="SAKT"></p>
<p>上图中，$x_{i}$经过interaction Embedding（通过Embedding Table ）得到embedding $\mathbf{M} \in \mathbb{R}^{2 E \times d}$，再经过Position Encodiing得到包含位置信息的embedding $\mathbb{P}$，最终输入Attention Layer的输入为$\hat{\mathbf{M}} = \mathbf{M} + \hat{\mathbf{P}}$。</p>
<p>而$e_{t}$通过Question Embedding（Embedding table）得到$\mathbf{E} \in \mathbb{R}^{E \times d}$,没有经过Position Encodiing，最终<br>$\hat{\mathbf{E}} = \mathbb{E}$。</p>
<p>在Attention Layer计算Attention score时，$\mathbf{Q}=\hat{\mathbf{E}} \mathbf{W}^{Q}, \mathbf{K}=\hat{\mathbf{M}} \mathbf{W}^{K}, \mathbf{V}=\hat{\mathbf{M}} \mathbf{W}^{V}$,即Key=Value‡Query。 与Transformer的 Query= Key= Value不同。</p>
<h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><p>论文最后对比了当前深度知识追踪中比较常见的集中模型，结果如下：</p>
<p> <img src="/2019/08/06/深度知识追踪/SAKT-2.jpg" alt="SAKT"></p>
<h3 id="感想："><a href="#感想：" class="headerlink" title="感想："></a>感想：</h3><p>早在Transformer发表之初，笔者就将Transformer引入知识追踪，所以上述文章的创新性感觉一般，但是值得思考的是为什么作者在计算attention score时Key‡Query,以及实现时，多层堆叠下的输入问题。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献:"></a>参考文献:</h2><ol>
<li><a href="https://arxiv.org/pdf/1506.05908.pdf" target="_blank" rel="noopener">Deep Knowledge Tracing</a></li>
<li><a href="https://arxiv.org/pdf/1806.02180.pdf" target="_blank" rel="noopener">Addressing Two Problems in Deep Knowledge Tracing via Prediction-Consistent Regularization</a></li>
<li><a href="http://www.educationaldatamining.org/EDM2016/proceedings/paper_133.pdf" target="_blank" rel="noopener">Going Deeper with Deep Knowledge Tracing </a></li>
<li><a href="https://arxiv.org/pdf/1604.02416.pdf" target="_blank" rel="noopener">How Deep is Knowledge Tracing?</a></li>
<li><a href="https://arxiv.org/pdf/1806.03256.pdf" target="_blank" rel="noopener">Incorporating Features Learned by an Enhanced Deep Knowledge Tracing Model for STEM/Non-STEM Job Prediction</a></li>
<li><a href="http://educationaldatamining.org/files/conferences/EDM2018/papers/EDM2018_paper_208.pdf" target="_blank" rel="noopener">Does Deep Knowledge Tracing Model Interactions Among Skills?</a></li>
<li><a href="https://aic-fe.bnu.edu.cn/docs/20190109161907836283.pdf" target="_blank" rel="noopener">Prerequisite-Driven Deep Knowledge Tracing</a></li>
<li><a href="https://arxiv.org/pdf/1907.06837.pdf" target="_blank" rel="noopener">A Self-Attentive model for Knowledge Tracing</a></li>
<li><a href="https://arxiv.org/pdf/1906.05658.pdf" target="_blank" rel="noopener">EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/06/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zoe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据挖掘日常笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/06/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-06T17:00:24+08:00">
                2019-08-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-06T17:00:24+08:00">
                2019-08-06
              </time>
            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  73
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Zoe">
            
              <p class="site-author-name" itemprop="name">Zoe</p>
              <p class="site-description motion-element" itemprop="description">Every failure is leading towards success.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zoe</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">13.1k</span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
